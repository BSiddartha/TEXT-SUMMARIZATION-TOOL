{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "183e48e6-4630-4f82-819a-3fce799fe6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac28c8be-2e25-48f4-b13e-c132ce4274e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sidda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sidda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d966c838-bbab-43a1-ad47-0d3f50ab8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, num_sentences=2):\n",
    "    try:\n",
    "        # Tokenize sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        \n",
    "        # Compute word frequencies\n",
    "        word_frequencies = {}\n",
    "        for word in word_tokenize(text.lower()):\n",
    "            if word.isalnum() and word not in stop_words:\n",
    "                word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
    "        \n",
    "        if not word_frequencies:\n",
    "            return \"No significant words to summarize.\"\n",
    "        \n",
    "        max_freq = max(word_frequencies.values())\n",
    "        for word in word_frequencies:\n",
    "            word_frequencies[word] /= max_freq\n",
    "        \n",
    "        # Score sentences\n",
    "        sentence_scores = {}\n",
    "        for sentence in sentences:\n",
    "            for word in word_tokenize(sentence.lower()):\n",
    "                if word in word_frequencies:\n",
    "                    sentence_scores[sentence] = sentence_scores.get(sentence, 0) + word_frequencies[word]\n",
    "        \n",
    "        if not sentence_scores:\n",
    "            return \"No meaningful sentences to summarize.\"\n",
    "        \n",
    "        # Select top sentences for summary\n",
    "        summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "        return \" \".join(summary_sentences)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8eb1055-a254-4608-9f79-1c350023ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "For those of you traditional data scientist who are interested in AI but still haven’t given it a deep dive, here’s a high level overview of the data science technologies that combine into what the popular press calls artificial intelligence (AI).\n",
    "\n",
    "2808322930We and others have written quite a bit about the various types of data science that make up AI.  Still I hear many folks asking about AI as if it were a single entity.  It is not.  AI is a collection of data science technologies that at this point in development are not even particularly well integrated or even easy to use.  In each of these areas however, we’ve made a lot of progress and that’s caught the attention of the popular press.\n",
    "\n",
    "This article is not intended to be a deep dive but rather the proverbial 50,000 foot view of what’s going on.  If you’re a traditional data scientist who’s read some articles but still hasn’t put the big picture together you might find this a way of integrating your current knowledge and even discovering where you’d be interested in focusing.\n",
    "\n",
    "AI is Simply the Sum of its Data Science Parts\n",
    "\n",
    "The data science ‘parts’ that make up AI fall in into the following categories.  There is overlap here but these are the detailed topics you’ll see in the press.\n",
    "\n",
    "Deep Learning\n",
    "Natural Language Processing\n",
    "Image Recognition\n",
    "Reinforcement Learning\n",
    "Question Answering Machines\n",
    "Adversarial Training\n",
    "Robotics\n",
    "These are all separate disciplines (OK the category of Deep Learning actually contains some of the others).  AI is simply the sum of these parts. They hang together only very loosely and have been bolted together into some really marvelous applications by a whole host of startups and major players.  When they work well together as they do for example in Watson, or Echo/Alexa, or as they are starting to do in self-driving cars then they may appear to be more than the sum of their parts.  But they’re not.  Integration of these different technologies is still one of the biggest challenges.\n",
    "\n",
    "What Must Our AI be Able to Do?\n",
    "\n",
    "2808332113When explaining this to beginners I always find it helpful to start with this anthropomorphic description of what human-like capabilities our AI would need to have.\n",
    "\n",
    "See:  this is still and video image recognition.\n",
    "\n",
    "Hear: receive input via text or spoken language.\n",
    "\n",
    "Speak:  respond meaningfully to our input either in the same language or even a foreign language.\n",
    "\n",
    "Make human-like decisions: Offer advice or new knowledge.\n",
    "\n",
    "Learn:  change its behavior based on changes in its environment.\n",
    "\n",
    "Move: and manipulate physical objects.\n",
    "\n",
    "You can immediately begin to see that many of the commercial applications of AI that are emerging today require only a few of these capabilities.  But the more sophisticated applications that we’re looking forward to would need to have pretty much all of these.\n",
    "\n",
    "Converting Human-Like Capabilities to Data Science\n",
    "\n",
    "Here’s where this gets a little messy.  Each of these capabilities don’t necessarily line up one-to-one with their underlying data science.  But how the data science matches up with these requirements is the most important part of truly understanding what’s going on in AI today.  As a diagram they would match up more or less like this:\n",
    "\n",
    "2808332549 \n",
    "\n",
    "What Happened to Deep Learning\n",
    "\n",
    "You may have noticed that ‘Deep Learning’ is missing from our chart.  That’s because it is a summary category for the Recurrent Neural Nets and Convolutional Neural Nets above.  Artificial Neural Nets (ANNs), the highest summary level have been around since the 80’s and have always been part of the standard data science machine learning tool kit for solving standard classification and regression problems.\n",
    "\n",
    " 2808321293\n",
    "\n",
    "What’s happened recently is that our massive increases in parallel processing, cloud processing, and the use of GPU (graphical processing units) instead of traditional Intel chips have allowed us to experiment with versions of ANNs that have dozens or even more than a hundred hidden layers.  These hidden layers are what causes us to call these types ‘deep’, hence ‘deep learning’.  Adding hidden layers means multiplying computational complexity which is why we had to wait for the hardware to catch up with our ambitions.\n",
    "\n",
    "There are at least 27 different types of ANNs but the most important are the Convolutional Neural Nets (CNNs) and the Recurrent Neural Nets (RNNs) without which image and natural language processing would not be possible.\n",
    "\n",
    "A (Very) Brief Discussion of the Data Science\n",
    "\n",
    "To do justice to any of these underlying data science technologies would require multiple articles.  What we’ll try to do here is give you the briefest of descriptions and some links to more complete information.\n",
    "\n",
    "Convolutional Neural Nets (CNNs):  CNNs are at the heart of all types of image and video recognition, facial recognition, image tagging (think Facebook) and recognizing a stop sign from a pedestrian in our self-driving cars.  They are extremely complex, difficult to train, and while you don’t need to specify specific features (the cat has fur, a tail, four legs, etc.) you do need to show a CNN literally millions of examples of cats before it can be successful.  The large amount of training data is a huge barrier.  See more about CNNs here.\n",
    "\n",
    "Recurrent Neural Nets (RNNs):  RNNs are at the center of natural language processing (NLP) and also game play and similar logical problems.  Unlike CNNs they process information as a time series in which each subsequent piece of data relies in some way on the piece that came before.  It may not be obvious but speech falls in this category since the next character or the next word is logically related to the preceding one.  RNNs can work at the character, word, or even long paragraph level which makes them perfect for providing the expected long form response to your customer service question.  RNNs handle both the understanding of the text question as well as the formulation of complex responses including translation into foreign languages.  RNNs are also responsible for computers winning at chess and Go.  See more about RNNs here.\n",
    "\n",
    "Generative Adversarial Neural Nets (GANNs):  CNNs and RNNs both suffer from the same problem of requiring huge and burdensome amounts of data in order to train, either to recognize that stop sign (image) or to learn the instructions necessary to answer your question about how to open that account (speech and text).  GANNs hold the promise of being able to dramatically reduce training data as well as dramatically increase accuracy.  They do it by battling each other.  There is a great story here about training a Convolutional Neural Net to recognize fake French impressionist paintings.  In a nutshell, one CNN is trained on real French impressionist paintings so that it is supposed to know the real ones.  The other adversarial CNN, called a Generative Adversarial Neural Net is given the task of actually creating fake impressionist painting. \n",
    "\n",
    "CNNs perform their task of image recognition by turning pixel values into complex numerical vectors.  If you run them backwards, that is start with random numerical vectors they can create an image.  In this scenario, the Generative NN creating fakes produces images trying to fool the CNN which is trying to learn how to detect fakes.  They battle it out until the Generative (fake maker) produces images so good the CNN can’t tell them from the original and the two adversarial nets draw to a tie.  Meanwhile the CNN designed to determine the originals from the fakes has been trained superbly in the detection of the fakes without the unrealistic requirement for millions of forged French impressionist masters on which to train.  In short this is learning from their environment.\n",
    "\n",
    "Question and Answer Machines (QAMs):  QAM is the rather inelegant name we give to the likes of IBM’s Watson.  These are huge knowledge repositories that are trained to find unique associations in their knowledge base and provide the answer to complex questions they have not previously seen.  Where ordinary search returns a list of sources where your answer might be found, QAMs must return the single best answer. \n",
    "\n",
    "This is a mash-up of NLP and sophisticated search in which the QAM builds multiple hypotheses about the possible meaning of the question and returns the best response based on weighted evidence algorithms.\n",
    "QAMs require large bodies of data about the topic to be studied to be loaded by humans and humans must then train and maintain the knowledge base.  However, once established they have been shown to be expert in areas such as cancer detection (in conjunction with CNNs), medical diagnosis, discovering unique combinations of materials and chemicals, and even in teaching your high school student how to program.  In short, wherever there is a very large body of knowledge that needs expert interpretation, a QAM can be the brain or at least the associative memory for our AI.  See some good references here, here, and here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "033de05c-9fa9-46de-a75e-50b8f85031d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "For those of you traditional data scientist who are interested in AI but still haven’t given it a deep dive, here’s a high level overview of the data science technologies that combine into what the popular press calls artificial intelligence (AI).\n",
      "\n",
      "2808322930We and others have written quite a bit about the various types of data science that make up AI.  Still I hear many folks asking about AI as if it were a single entity.  It is not.  AI is a collection of data science technologies that at this point in development are not even particularly well integrated or even easy to use.  In each of these areas however, we’ve made a lot of progress and that’s caught the attention of the popular press.\n",
      "\n",
      "This article is not intended to be a deep dive but rather the proverbial 50,000 foot view of what’s going on.  If you’re a traditional data scientist who’s read some articles but still hasn’t put the big picture together you might find this a way of integrating your current knowledge and even discovering where you’d be interested in focusing.\n",
      "\n",
      "AI is Simply the Sum of its Data Science Parts\n",
      "\n",
      "The data science ‘parts’ that make up AI fall in into the following categories.  There is overlap here but these are the detailed topics you’ll see in the press.\n",
      "\n",
      "Deep Learning\n",
      "Natural Language Processing\n",
      "Image Recognition\n",
      "Reinforcement Learning\n",
      "Question Answering Machines\n",
      "Adversarial Training\n",
      "Robotics\n",
      "These are all separate disciplines (OK the category of Deep Learning actually contains some of the others).  AI is simply the sum of these parts. They hang together only very loosely and have been bolted together into some really marvelous applications by a whole host of startups and major players.  When they work well together as they do for example in Watson, or Echo/Alexa, or as they are starting to do in self-driving cars then they may appear to be more than the sum of their parts.  But they’re not.  Integration of these different technologies is still one of the biggest challenges.\n",
      "\n",
      "What Must Our AI be Able to Do?\n",
      "\n",
      "2808332113When explaining this to beginners I always find it helpful to start with this anthropomorphic description of what human-like capabilities our AI would need to have.\n",
      "\n",
      "See:  this is still and video image recognition.\n",
      "\n",
      "Hear: receive input via text or spoken language.\n",
      "\n",
      "Speak:  respond meaningfully to our input either in the same language or even a foreign language.\n",
      "\n",
      "Make human-like decisions: Offer advice or new knowledge.\n",
      "\n",
      "Learn:  change its behavior based on changes in its environment.\n",
      "\n",
      "Move: and manipulate physical objects.\n",
      "\n",
      "You can immediately begin to see that many of the commercial applications of AI that are emerging today require only a few of these capabilities.  But the more sophisticated applications that we’re looking forward to would need to have pretty much all of these.\n",
      "\n",
      "Converting Human-Like Capabilities to Data Science\n",
      "\n",
      "Here’s where this gets a little messy.  Each of these capabilities don’t necessarily line up one-to-one with their underlying data science.  But how the data science matches up with these requirements is the most important part of truly understanding what’s going on in AI today.  As a diagram they would match up more or less like this:\n",
      "\n",
      "2808332549 \n",
      "\n",
      "What Happened to Deep Learning\n",
      "\n",
      "You may have noticed that ‘Deep Learning’ is missing from our chart.  That’s because it is a summary category for the Recurrent Neural Nets and Convolutional Neural Nets above.  Artificial Neural Nets (ANNs), the highest summary level have been around since the 80’s and have always been part of the standard data science machine learning tool kit for solving standard classification and regression problems.\n",
      "\n",
      " 2808321293\n",
      "\n",
      "What’s happened recently is that our massive increases in parallel processing, cloud processing, and the use of GPU (graphical processing units) instead of traditional Intel chips have allowed us to experiment with versions of ANNs that have dozens or even more than a hundred hidden layers.  These hidden layers are what causes us to call these types ‘deep’, hence ‘deep learning’.  Adding hidden layers means multiplying computational complexity which is why we had to wait for the hardware to catch up with our ambitions.\n",
      "\n",
      "There are at least 27 different types of ANNs but the most important are the Convolutional Neural Nets (CNNs) and the Recurrent Neural Nets (RNNs) without which image and natural language processing would not be possible.\n",
      "\n",
      "A (Very) Brief Discussion of the Data Science\n",
      "\n",
      "To do justice to any of these underlying data science technologies would require multiple articles.  What we’ll try to do here is give you the briefest of descriptions and some links to more complete information.\n",
      "\n",
      "Convolutional Neural Nets (CNNs):  CNNs are at the heart of all types of image and video recognition, facial recognition, image tagging (think Facebook) and recognizing a stop sign from a pedestrian in our self-driving cars.  They are extremely complex, difficult to train, and while you don’t need to specify specific features (the cat has fur, a tail, four legs, etc.) you do need to show a CNN literally millions of examples of cats before it can be successful.  The large amount of training data is a huge barrier.  See more about CNNs here.\n",
      "\n",
      "Recurrent Neural Nets (RNNs):  RNNs are at the center of natural language processing (NLP) and also game play and similar logical problems.  Unlike CNNs they process information as a time series in which each subsequent piece of data relies in some way on the piece that came before.  It may not be obvious but speech falls in this category since the next character or the next word is logically related to the preceding one.  RNNs can work at the character, word, or even long paragraph level which makes them perfect for providing the expected long form response to your customer service question.  RNNs handle both the understanding of the text question as well as the formulation of complex responses including translation into foreign languages.  RNNs are also responsible for computers winning at chess and Go.  See more about RNNs here.\n",
      "\n",
      "Generative Adversarial Neural Nets (GANNs):  CNNs and RNNs both suffer from the same problem of requiring huge and burdensome amounts of data in order to train, either to recognize that stop sign (image) or to learn the instructions necessary to answer your question about how to open that account (speech and text).  GANNs hold the promise of being able to dramatically reduce training data as well as dramatically increase accuracy.  They do it by battling each other.  There is a great story here about training a Convolutional Neural Net to recognize fake French impressionist paintings.  In a nutshell, one CNN is trained on real French impressionist paintings so that it is supposed to know the real ones.  The other adversarial CNN, called a Generative Adversarial Neural Net is given the task of actually creating fake impressionist painting. \n",
      "\n",
      "CNNs perform their task of image recognition by turning pixel values into complex numerical vectors.  If you run them backwards, that is start with random numerical vectors they can create an image.  In this scenario, the Generative NN creating fakes produces images trying to fool the CNN which is trying to learn how to detect fakes.  They battle it out until the Generative (fake maker) produces images so good the CNN can’t tell them from the original and the two adversarial nets draw to a tie.  Meanwhile the CNN designed to determine the originals from the fakes has been trained superbly in the detection of the fakes without the unrealistic requirement for millions of forged French impressionist masters on which to train.  In short this is learning from their environment.\n",
      "\n",
      "Question and Answer Machines (QAMs):  QAM is the rather inelegant name we give to the likes of IBM’s Watson.  These are huge knowledge repositories that are trained to find unique associations in their knowledge base and provide the answer to complex questions they have not previously seen.  Where ordinary search returns a list of sources where your answer might be found, QAMs must return the single best answer. \n",
      "\n",
      "This is a mash-up of NLP and sophisticated search in which the QAM builds multiple hypotheses about the possible meaning of the question and returns the best response based on weighted evidence algorithms.\n",
      "QAMs require large bodies of data about the topic to be studied to be loaded by humans and humans must then train and maintain the knowledge base.  However, once established they have been shown to be expert in areas such as cancer detection (in conjunction with CNNs), medical diagnosis, discovering unique combinations of materials and chemicals, and even in teaching your high school student how to program.  In short, wherever there is a very large body of knowledge that needs expert interpretation, a QAM can be the brain or at least the associative memory for our AI.  See some good references here, here, and here.\n",
      "\n",
      "\n",
      "Summarized Text:\n",
      "\n",
      "For those of you traditional data scientist who are interested in AI but still haven’t given it a deep dive, here’s a high level overview of the data science technologies that combine into what the popular press calls artificial intelligence (AI). Generative Adversarial Neural Nets (GANNs):  CNNs and RNNs both suffer from the same problem of requiring huge and burdensome amounts of data in order to train, either to recognize that stop sign (image) or to learn the instructions necessary to answer your question about how to open that account (speech and text).\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nSummarized Text:\")\n",
    "print(summarize_text(text, num_sentences=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93699887-875e-4a26-a68d-2022ed05142e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
